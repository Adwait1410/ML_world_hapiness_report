# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1suKRpVrgSQYtjadr87xcmh00N3b897wk
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/home/pccoe/Downloads/2015.csv")

df

df.shape

df.info()

df.describe()

df.describe(include='all')

df.isnull().sum()

df.duplicated().sum()

from sklearn.impute import KNNImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif
from sklearn.decomposition import PCA
from scipy import stats

#outlier detection
Q1 = df['Happiness Score'].quantile(0.25)
Q3 = df['Happiness Score'].quantile(0.75)
IQR = Q3 - Q1
# Determine bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Identify outliers
outliers_Happiness_Score = df[(df['Happiness Score'] < lower_bound) | (df['Happiness Score'] > upper_bound)]
print(f"Number of outliers in Happiness Score: {len(outliers_Happiness_Score)}")

Q1 = df['Standard Error'].quantile(0.25)
Q3 = df['Standard Error'].quantile(0.75)
IQR = Q3 - Q1
# Determine bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Identify outliers
outliers_Standard_Error = df[(df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound)]
print(f"Number of outliers in Standard Error: {len(outliers_Standard_Error)}")

df = df[~((df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound))]

Q1 = df['Standard Error'].quantile(0.25)
Q3 = df['Standard Error'].quantile(0.75)
IQR = Q3 - Q1
# Determine bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Identify outliers
outliers_Standard_Error = df[(df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound)]
print(f"Number of outliers in Standard Error: {len(outliers_Standard_Error)}")

df = df[~((df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound))]

Q1 = df['Standard Error'].quantile(0.25)
Q3 = df['Standard Error'].quantile(0.75)
IQR = Q3 - Q1
# Determine bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Identify outliers
outliers_Standard_Error = df[(df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound)]
print(f"Number of outliers in Standard Error: {len(outliers_Standard_Error)}")

df = df[~((df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound))]

Q1 = df['Standard Error'].quantile(0.25)
Q3 = df['Standard Error'].quantile(0.75)
IQR = Q3 - Q1
# Determine bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Identify outliers
outliers_Standard_Error = df[(df['Standard Error'] < lower_bound) | (df['Standard Error'] > upper_bound)]
print(f"Number of outliers in Standard Error: {len(outliers_Standard_Error)}")

#Step : Label Encoding of 'Sex'
#convert categorical data to numerical
le = LabelEncoder()
df['Region'] = le.fit_transform(df['Region']) # male:1, female:0

#to check if region is converted or not into numerical from categorical from labelled encoding
df.head()

#Step : One-Hot Encoding 'Embarked'
df = pd.get_dummies(df, columns=['Country'], drop_first=True)

df.head()

df

#Step 13: Plot Age Distribution
plt.hist(df['Happiness Score'], bins=30, color='skyblue', edgecolor='black')
plt.title("Histogram of Happiness Score")
plt.xlabel("Happiness Rank")
plt.ylabel("Happiness Score")
plt.show()

#Step 14: Plot Box Plot for Age vs. Pclass
sns.boxplot(data=df, x='Health (Life Expectancy)', y='Freedom')
plt.title("Boxplot")
plt.show()

corr_matrix = df.corr()
print(corr_matrix)

plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

#feature selection
selector = VarianceThreshold(threshold=0.01)
numeric_df = df.select_dtypes(include=[np.number])
X_reduced = selector.fit_transform(numeric_df)
print("Shape after variance threshold:", X_reduced.shape)

#Step 18: Feature Selection – Chi-Square
from sklearn.feature_selection import SelectKBest, f_regression

X = numeric_df.drop(columns=['Family'])
y = numeric_df['Family']

selector = SelectKBest(score_func=f_regression, k=5)
X_kbest = selector.fit_transform(X, y)

selected_features = X.columns[selector.get_support()]
print("Top 5 features by f_regression:", selected_features)

# Feature Selection – Mutual Information
from sklearn.feature_selection import mutual_info_regression

mi_scores = mutual_info_regression(X, y)
mi_series = pd.Series(mi_scores, index=X.columns).sort_values()
print("Mutual Information Scores:\n", mi_series)

